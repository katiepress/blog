---
title: Trust No One
author: Katie Press
date: '2021-01-03'
slug: trust-no-one
categories:
  - Projects
tags:
  - Web Scraping
  - X-Files
  - Datasets
autoThumbnailImage: false
thumbnailImage: https://lh3.googleusercontent.com/vRq1TRXmDWNmVsDybQYGKOILgT6qVcHfzctGj2KBjhPxf6_7yUKCNTxWH5LmiAPNPlecvqTjbQ2bW-Q7kySUk-4iAl_pMmbUazeLodjPeRm8dmIjVRRWjrH9sZgD6FeBfP8YbGbOD21JKiPvUyL3qz5MdggQTPFOZwO2Omzk_fmhSHsH6y_1V1IIYZZIup2Z6gh9nPXec_omBvtvaJl4pSYh6fXPgDhfAWmmGKpANhs7xeruxmEaxea1iwseSreZr_c6hz5O8lb3wZI78eXJfoQcjSz6onJnl5CCCr_yzlFvyifM7GLw61iuBAaxtoZX9n2rkSi4JSgoqsbLKIyT4KBb8IdtfvAE8gwDo2nNYnJ1Xr6FMAsTKd7ya9cw3sg3IRnq4V8KOGwHvIpMqFKQoxQvTTvc8r06nrvD5wLYlj1JgdL0W_fP0-7bFr1zKPrP7akqwBcRH3pVXg1CgfCO5GF733oEZw2DH6ePSNGQyVO8uk_kMhvyFgag7JmGbdVlUCOKa1WZb47cas6PR8CsoFj-gNeTV9KNyJCJ9czJb4M-vCINxR0NsPiYOa5fGbZLUCJ4LUugruuKx6WbshsnjnXEliy3j401yx3CSiR7TtwZ37a6Ce2guRNcAEYB_0fi3aAasBSFtqwicm7NKTo-KNMFEQeVQHtHQ88oq5I_SpiZXDg_v_RbvL7q40P4TBL9rH2zRqF9sqIb_Bh5Fr3TIVg=w750-h250-no?authuser=0
thumbnailImagePosition: bottom
coverImage: https://lh3.googleusercontent.com/MinWvwDIPcTmNiBtEObOecczDyieH7jFS0c5Yr7D3z3vQ9z4gX3ZWJtnb74LarxuHOkxpwVxj4SVXYIZpzIt3B9J4n341XPpxYnFlqHC_Evm4yy7GkvBMqkXZVUvfbf-Ne6ZbSOykdKA7jmoPd3ocVUx9Ifeeb17nPd9YbjTGA8ftPNxcGH5qMcCDzyUk7i4IwaR31xOQ4uTaBA_1U2vCEpR81driw2uyJH2hipk_iqkdpHGclzpMDwm12ox3-WXqRLZP32Nl95kqS4hO1QirHk7ypD22ItsgWY_PlpirHwqSrScyMNO6Cb2fOYrPzcl4rbuLuPhQsALTQEC810ArFcGEQXcnURm0z4XnElIZ4jJtSHSJ7whGhIXQecLlshGZtm-JYAmvdvnESlmoBPo9LQ_VMQi0KxJp5uqK-cYT3q77gKUSC9s5u2UjufgNBGNXysz5Om32Lua-NLzgUptx-GtyiUqp7UPiHhN09p_h5xO-bC7e_JtOSwMKKS_r4b2fYee8SDjjLYzevVlXWNH09287KZUfneyzBxMAmfet4LZjTAniFM8EX3qwPzz9Hb2DQXHZYOPYd7p7F4D8oJIBgm7kC0QNwCXU9iudk3QDQU3CyTjAmpYLFdOZyrZIuIGysTCtDWE6YLu7o-yfIR1Ojqm8lfZgnigi6dvOZteKxj0l6uLXyyr_OXQQuyricuhLocaQNqiYv3XGC0cUE-YJ1E=w1303-h868-no?authuser=0
metaAlignment: center
summary: "Sentiment analysis of X-Files scripts using tidytext"
---

<!--more-->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidytext)
library(lubridate)
library(janitor)
library(ggplot2)
library(extrafont)
library(textdata)
library(dashboardthemes)
library(ggplot2)
library(treemapify)
library(reactable)
#font_import()

loadfonts()
```


```{r include=FALSE}
ep_text <- readRDS("~/Desktop/blog2/content/post/2020-11-01-the-truth-is-out-there-part-ii/ep_text.RDS")

 table_df <- readRDS("~/Desktop/blog2/content/post/2020-10-04-the-truth-is-out-there/table_df.RDS")
```

In the last two episodes, I created an [X-Files episode dataset](https://kpress.dev/blog/2021-01-03-the-truth-is-out-there/) and [web-scraped episode scripts](https://kpress.dev/blog/2020-10-04-the-truth-is-out-there-part-ii/). Now it's time for some real text analysis. Most of this is going to be based on Julia Silge's work on tidy text mining, which can be found in her [free eBook](https://www.tidytextmining.com/). And of course, all of this is based on the principles of tidy data - see [this paper](https://vita.had.co.nz/papers/tidy-data.pdf) by tidyverse author Hadley Wickham for more details.

## Tidy Text Data

Basically, tidy data is set up so that each variable is in its own column, each observation of said variable has its own row, and these combine to make a table. This is sometimes referred to as "long" data. The tidytext packages has lots of helpful functions for manipuating and analyzing text data in a tidy way. I much prefer this to using a corpus because I can use all of the tidyverse functions I usually use.

The first thing I'm going to do to create a tidy dataset is to use the function unnest_tokens so that I can have one word on every line. If you guessed that this will make the dataset huge, you are correct. But that's okay, because it's in long format and long format data is generally faster and easier to work with from a system/memory standpoint. However, I will get rid of a couple of extra columns I know I don't need.

```{r}
ep_words <- ep_text %>% 
  mutate(line_no = row_number()) %>% 
  select(-description, -prod_code, -text, -directed_by, -written_by) %>% 
  unnest_tokens(., "word", line) %>% 
  mutate(word = str_to_lower(word))
```

Now there will be some words that I don't want to use that would just clutter up any analysis I try to do. These are mostly filler words like "the", "and", "if". Thankfully tidytext has a built-in dataset of these words, called stop_words. I can use anti_join to get rid of all the stop words from my dataset. This cuts it down by about a third, so there were quite a lot of them! 

```{r}
ep_words <- anti_join(ep_words, stop_words)
```

The most common word is Mulder, which makes sense. But Scully is fifth, behind phone, agent, and "yeah". 

```{r echo=FALSE}
reactable(count(ep_words, word, sort= TRUE),
          bordered = TRUE,
                     highlight = TRUE,
                     striped = TRUE,
                     compact = TRUE,
                     wrap = FALSE
          )
```
## Sentiment Analysis

The tidytext package provides four different sentiment datasets. One is numeric, one is divided into negative/positive, and the last two a sentiment to each word, such as "fear" or "trust".

```{r}
afinn <- get_sentiments("afinn")
bing <- get_sentiments("bing")
nrc <- get_sentiments("nrc")
lran <- get_sentiments("loughran")

main.characters <- c("Scully", "Mulder", "Cigarette Smoking Man", "X", "Skinner", 
                     "Frohike", "Byers", "Langly")
```


```{r include=FALSE}
my_theme <- theme(legend.position = "none",
        plot.title = element_text(family = "Arial", size = 14, color = "#232928", hjust = .5),
        axis.text.y = element_text(family = "Arial", size = 11, color = "#232928"),
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank(),
        axis.title.y = element_blank(),
        axis.title.x = element_text(family = "Arial", size = 11, vjust = -.5, color= "#232928"),
        panel.background = element_rect(color = "#AFAEB2", fill="#232928"
                                        ),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        strip.background = element_rect(color = "#232928"))
```


Using inner_join so that only the words that exist in both datasets will be included, I can look at the sentiments of the main characters. Overall, the text does have a negative tone for all speakers. It looks like Frohike is winnning the positivity contest by a landslide though, which checks out.

```{r message=FALSE, warning=FALSE}
tb1 <- ep_words %>% 
  filter(speaker %in% main.characters) %>% 
  inner_join(bing) %>% 
  group_by(speaker) %>% 
  count(sentiment) %>% 
  arrange(speaker, sentiment) %>% 
  mutate(n = scales::percent(n/sum(n))) %>% 
  spread(sentiment, n)
```

```{r echo=FALSE}
reactable(tb1, 
          bordered = TRUE,
                     highlight = TRUE,
                     striped = TRUE,
                     compact = TRUE,
                     wrap = FALSE)
```


I'm interested to see what the nrc and loughran sentiment datasets look like for the main characters as well. It looks like there are 10 sentiments in the nrc dataset, and only 6 in the loughran dataset. Of those 6, I realized that most of the words in my dataset fall into the "negative" category, which makes it pretty much unusable for my purposes.

#### NRC Words
```{r echo=FALSE}
reactable(count(nrc, sentiment)
  ,
  bordered = TRUE,
                     highlight = TRUE,
                     striped = TRUE,
                     compact = TRUE,
                     wrap = FALSE
)
```

#### LRAN Words
```{r echo=FALSE}
reactable(count(lran, sentiment),
          bordered = TRUE,
                     highlight = TRUE,
                     striped = TRUE,
                     compact = TRUE,
                     wrap = FALSE)
```

When I join the nrc dataset, it looks like more than one sentiment can be tied to each word, which is interesting.

```{r}
nrc_main <- ep_words %>% 
  filter(speaker %in% main.characters) %>% 
  inner_join(nrc) %>% 
  select(season, speaker, word, sentiment)
```

```{r echo=FALSE}
reactable(head(nrc_main)
  ,
  bordered = TRUE,
                     highlight = TRUE,
                     striped = TRUE,
                     compact = TRUE,
                     wrap = FALSE
)
```


Let's compare the sentiments of the main speakers. Apparently X is never surprised or joyful, and it looks like Byers is the most trusting ... again, that checks out.

```{r}
my_theme3 <- theme(legend.position = "none",
        plot.title = element_text(family = "Arial", size = 14, color = "#232928", hjust = .5),
        axis.title = element_blank(),
        axis.text.y = element_text(family = "Arial", size = 11, color = "#232928"),
        axis.text.x = element_text(family = "Arial", size = 11, color = "#232928", angle = 45, hjust = 1)
        )
```


```{r fig.width=7.5, fig.height=6.5}
nrc_main %>% 
  count(speaker, sentiment) %>% 
  group_by(speaker) %>% 
  mutate(n = n/sum(n),
         sentiment =str_to_title(sentiment)) %>% 
  ggplot(aes(x = sentiment, y = fct_rev(speaker)))+
  geom_count(aes(size = n, group = speaker, color = sentiment),show.legend = FALSE) +
  scale_size(range = c(0,10)) +
  coord_fixed() +
  theme_minimal()+
  my_theme3 +
  ggtitle("Sentiment by Character")
```

```{r eval=FALSE, include=FALSE}
ggsave("~/Desktop/site/static/img/2021/trust_no_0.png", dev="png", dpi = 200, width = 7.5, height =6.5)
```

I'm also interested in looking at sentiment by season and/or episode, and I think I'll use the numeric sentiments for this. The afinn dataset categorizes words from -5 (most negative sentiment) to 5 (most positive sentiment). I'm going to create a dataset of all words from ep_words that match to afinn. I'm basically going to chunk the episodes within each season and produce the overall sentiment for each chunk so that I can chart it. First divide every episode into four equal portions, this can sort of represent 15 minute chunks of each episode.

```{r}
season_sent <- ep_words %>% 
  inner_join(afinn) %>% 
  group_by(no_overall) %>% 
  mutate(line_no2 = data.table::rleid(line_no))  %>% 
  group_by(no_overall) %>% 
  mutate(index = cut(line_no2, 4, include.lowest = TRUE, labels = c(1, 2,3,4))) %>% 
  group_by(season, no_overall, index) %>% 
  summarise(sentiment = sum(value)) %>% 
  mutate(final_index = paste0(no_overall, index))
```

```{r include=FALSE}
my_theme2 <- theme(legend.position = "none",
        plot.title = element_text(family = "Arial", size = 14, color = "#232928", hjust = .5),
        axis.text.y = element_text(family = "Arial", size = 11, color = "#232928"),
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank(),
        axis.title.y = element_blank(),
        axis.title.x = element_text(family = "Arial", size = 11, vjust = -.5, color= "#232928"),
        panel.background = element_rect(color = "#AFAEB2", fill="#232928"
                                        ),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        strip.background = element_rect(color = "#232928"))
```

As you can see below, most of Mulder and Scully's time investigating the X-Files was not exactly positive...but there were still some good moments. 

```{r}
ggplot(season_sent, aes(final_index, sentiment, fill = season)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~season, ncol = 3, scales = "free_x") +
  my_theme2 +
  theme(axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.title.x = element_blank())+
  ggtitle("Sentiment Change Over Time")
```

```{r eval=FALSE, include=FALSE}
ggsave("~/Desktop/site/static/img/2021/trust_no.png", dev="png", dpi = 200, width = 7.5, height =6.5)
```

I think this would look pretty cool in a single chart too so let's try that next.
```{r message=FALSE, warning=FALSE}
final_sent <- ep_words %>% 
  inner_join(afinn) %>% 
  mutate(line_no2 = data.table::rleid(line_no))  %>% 
  group_by(season, no_inseason) %>% 
  summarise(sentiment = sum(value)) %>% 
  ungroup() %>% 
  mutate(final_index = as.factor(row_number())) %>% 
  ungroup()
```


```{r}
ggplot(final_sent, aes(final_index, sentiment,fill=season)) +
  geom_col(show.legend = FALSE) +
  my_theme2 +
  theme(axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.title.x = element_blank())+
  ggtitle("Sentiment Change Over Time")
```

That's all for now...next time might be topic modeling. 

```{r eval=FALSE, include=FALSE}
ggsave("~/Desktop/site/static/img/2021/trust_no_1.png", dev="png", dpi = 200, width = 7.5, height =5.5)
```

